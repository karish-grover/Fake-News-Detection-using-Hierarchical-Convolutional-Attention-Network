{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HCAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qktAMHpzO2Pa"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "from keras import backend as K\n",
        "from tensorflow.python.keras.layers import Layer, InputSpec\n",
        "from keras import initializers\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eqpskII9k9n",
        "outputId": "c8ec05ed-29d7-40c5-f4b4-6f05f73f9dcd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4SnGHl5j3X0"
      },
      "source": [
        "**TextAttnBiRNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaB7M_Demdeh",
        "outputId": "4bc9638e-8231-4e1f-bf6e-723b0f1b055b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 27 16:25:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R7yoMOYkNFu",
        "outputId": "ba777ecb-2185-4a62-b2af-9911d2b6540c"
      },
      "source": [
        "cd /content/gdrive/MyDrive/College/Semester5/NLP/project"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/College/Semester5/NLP/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYz1pecgk6_M"
      },
      "source": [
        "# !pip install --upgrade tensorflow"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak07QYnM-QQC"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            # 1\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "            # 2\n",
        "            hidden = LSTM(64, return_sequences=True)(words)\n",
        "            sentence = Attention()(hidden)\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
        "                                 shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
        "                                     shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
        "        if self.bias:\n",
        "            e += self.b\n",
        "        e = K.tanh(e)\n",
        "\n",
        "        a = K.exp(e)\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        a = K.expand_dims(a)\n",
        "\n",
        "        c = K.sum(a * x, axis=1)\n",
        "        return c\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.features_dim\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYDReXrA-K8f"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Bidirectional\n",
        "from keras.layers import CuDNNLSTM\n",
        "import tensorflow.keras.layers\n",
        "\n",
        "class TextAttBiRNN(Model):\n",
        "    def __init__(self,\n",
        "                 maxlen,\n",
        "                 max_features,\n",
        "                 embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        super(TextAttBiRNN, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        self.embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
        "        self.bi_rnn = Bidirectional(CuDNNLSTM(128, return_sequences=True))  # LSTM or GRU\n",
        "        # self.conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")\n",
        "        # self.pooling_layer = layers.GlobalMaxPool1D()\n",
        "\n",
        "        self.attention = Attention(self.maxlen)\n",
        "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.get_shape()) != 2:\n",
        "            raise ValueError('The rank of inputs of TextAttBiRNN must be 2, but now is %d' % len(inputs.get_shape()))\n",
        "        if inputs.get_shape()[1] != self.maxlen:\n",
        "            raise ValueError('The maxlen of inputs of TextAttBiRNN must be %d, but now is %d' % (self.maxlen, inputs.get_shape()[1]))\n",
        "        embedding = self.embedding(inputs)\n",
        "        # x = self.conv_layer(embedding)\n",
        "        # x = self.pooling_layer(x)\n",
        "        x = self.bi_rnn(embedding)\n",
        "        x = self.attention(x)\n",
        "        output = self.classifier(x)\n",
        "        return output\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBbRfG9ckUlS",
        "outputId": "085a0027-8440-48ff-ffd4-4bb1f9a18108"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import pandas as pd\n",
        "import numpy\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn import metrics\n",
        "max_features = 5000\n",
        "maxlen = 70\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "\n",
        "\n",
        "print('Loading data...')\n",
        "train = pd.read_csv(\"/content/gdrive/MyDrive/College/Semester5/NLP/project/han_train.csv\")\n",
        "test = pd.read_csv(\"/content/gdrive/MyDrive/College/Semester5/NLP/project/han_test.csv\")\n",
        "print(train.columns)\n",
        "print(test.columns)\n",
        "x_train, y_train, x_test, y_test = train['message'], train['class'], test['message'], test['class']\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)...')\n",
        "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('/content/gdrive/MyDrive/Amazon/productner/data/glove.6B.100d.txt')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(train['message'])\n",
        "word_index = token.word_index\n",
        "\n",
        "x_train = sequence.pad_sequences(token.texts_to_sequences(x_train), maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(token.texts_to_sequences(x_test), maxlen=maxlen)\n",
        "\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Index(['id', 'title', 'author', 'message', 'class'], dtype='object')\n",
            "Index(['id', 'title', 'author', 'message', 'class', 'predicted'], dtype='object')\n",
            "16640 train sequences\n",
            "4160 test sequences\n",
            "Pad sequences (samples x time)...\n",
            "x_train shape: (16640, 70)\n",
            "x_test shape: (4160, 70)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EB9wUe1tdrt",
        "outputId": "3acdc9cd-f310-4df5-fa78-625db7968c84"
      },
      "source": [
        "epochs = 4\n",
        "print('Build model...')\n",
        "model = TextAttBiRNN(maxlen, max_features, embedding_dims)\n",
        "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[early_stopping],\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "print('Test...')\n",
        "result = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Epoch 1/4\n",
            "520/520 [==============================] - 31s 49ms/step - loss: 0.2424 - accuracy: 0.8957 - val_loss: 0.1877 - val_accuracy: 0.9212\n",
            "Epoch 2/4\n",
            "520/520 [==============================] - 25s 49ms/step - loss: 0.1118 - accuracy: 0.9587 - val_loss: 0.1753 - val_accuracy: 0.9327\n",
            "Epoch 3/4\n",
            "520/520 [==============================] - 26s 51ms/step - loss: 0.0741 - accuracy: 0.9741 - val_loss: 0.1707 - val_accuracy: 0.9341\n",
            "Epoch 4/4\n",
            "520/520 [==============================] - 26s 51ms/step - loss: 0.0441 - accuracy: 0.9835 - val_loss: 0.2458 - val_accuracy: 0.9248\n",
            "Test...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTtckxNYsxeK"
      },
      "source": [
        "result = (result.reshape(1,-1)[0]>0.5).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wlmM9imrFp_",
        "outputId": "fd677e89-34a1-4e40-c585-6d3b6acc9a3b"
      },
      "source": [
        "print(\"ACC\", metrics.accuracy_score(result, y_test))\n",
        "print(\"Prec\", metrics.precision_score(result, y_test))\n",
        "print(\"REC\", metrics.recall_score(result, y_test))\n",
        "print(\"F1\", metrics.f1_score(result, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC 0.9247596153846154\n",
            "Prec 0.9297501178689298\n",
            "REC 0.9232209737827716\n",
            "F1 0.9264740427531125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SPXxbDzvCyY"
      },
      "source": [
        "**HCAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNSzBsq5-pd9"
      },
      "source": [
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Bidirectional, TimeDistributed\n",
        "# from keras.layers import CuDNNGRU\n",
        "from keras.layers import GRU\n",
        "\n",
        "class HCAN(Model):\n",
        "    def __init__(self,\n",
        "                 maxlen_sentence,\n",
        "                 maxlen_word,\n",
        "                 max_features,\n",
        "                 embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        super(HCAN, self).__init__()\n",
        "        self.maxlen_sentence = maxlen_sentence\n",
        "        self.maxlen_word = maxlen_word\n",
        "        self.max_features = max_features\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "        # Word part\n",
        "        input_word = Input(shape=(self.maxlen_word,))\n",
        "        x_word = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen_word)(input_word)\n",
        "        x_word = layers.Convolution1D(100, 10, activation=\"relu\", padding = 'same')(x_word)\n",
        "        # x_word = layers.GlobalMaxPool1D()(x_word)\n",
        "\n",
        "        x_word = Bidirectional(GRU(128, return_sequences=True))(x_word)  # LSTM or GRU\n",
        "        x_word = Attention(self.maxlen_word)(x_word)\n",
        "        model_word = Model(input_word, x_word)\n",
        "        # Sentence part\n",
        "        self.word_encoder_att = TimeDistributed(model_word)\n",
        "        self.sentence_encoder = Bidirectional(GRU(128, return_sequences=True))  # LSTM or GRU\n",
        "        self.sentence_att = Attention(self.maxlen_sentence)\n",
        "        # Output part\n",
        "        self.classifier = Dense(self.class_num, activation=self.last_activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.get_shape()) != 3:\n",
        "            raise ValueError('The rank of inputs of HAN must be 3, but now is %d' % len(inputs.get_shape()))\n",
        "        if inputs.get_shape()[1] != self.maxlen_sentence:\n",
        "            raise ValueError('The maxlen_sentence of inputs of HAN must be %d, but now is %d' % (self.maxlen_sentence, inputs.get_shape()[1]))\n",
        "        if inputs.get_shape()[2] != self.maxlen_word:\n",
        "            raise ValueError('The maxlen_word of inputs of HAN must be %d, but now is %d' % (self.maxlen_word, inputs.get_shape()[2]))\n",
        "        x_sentence = self.word_encoder_att(inputs)\n",
        "        x_sentence = self.sentence_encoder(x_sentence)\n",
        "        x_sentence = self.sentence_att(x_sentence)\n",
        "        output = self.classifier(x_sentence)\n",
        "        return output\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uw2cU0TvGRf",
        "outputId": "7d6e44dd-6d96-4a9b-9b71-c539faa31caa"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import pandas as pd\n",
        "import numpy\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn import metrics\n",
        "\n",
        "max_features = 5000\n",
        "maxlen_sentence = 16\n",
        "maxlen_word = 25\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "\n",
        "\n",
        "print('Loading data...')\n",
        "train = pd.read_csv(\"/content/gdrive/MyDrive/College/Semester5/NLP/project/han_train.csv\")\n",
        "test = pd.read_csv(\"/content/gdrive/MyDrive/College/Semester5/NLP/project/han_test.csv\")\n",
        "print(train.columns)\n",
        "print(test.columns)\n",
        "x_train, y_train, x_test, y_test = train['message'], train['class'], test['message'], test['class']\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x #sentence x #word)...')\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('/content/gdrive/MyDrive/Amazon/productner/data/glove.6B.100d.txt')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(train['message'])\n",
        "word_index = token.word_index\n",
        "\n",
        "x_train = sequence.pad_sequences(token.texts_to_sequences(x_train),  maxlen=maxlen_sentence * maxlen_word)\n",
        "x_test = sequence.pad_sequences(token.texts_to_sequences(x_test), maxlen=maxlen_sentence * maxlen_word)\n",
        "\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "x_train = x_train.reshape((len(x_train), maxlen_sentence, maxlen_word))\n",
        "x_test = x_test.reshape((len(x_test), maxlen_sentence, maxlen_word))\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Index(['id', 'title', 'author', 'message', 'class'], dtype='object')\n",
            "Index(['id', 'title', 'author', 'message', 'class', 'predicted'], dtype='object')\n",
            "16640 train sequences\n",
            "4160 test sequences\n",
            "Pad sequences (samples x #sentence x #word)...\n",
            "x_train shape: (16640, 16, 25)\n",
            "x_test shape: (4160, 16, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YQSKK2i_qpb"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIMPk7f_wda3",
        "outputId": "8f5e8306-0aac-44a1-fd3a-f91efeeaba80"
      },
      "source": [
        "epochs = 5\n",
        "print('Build model...')\n",
        "model = HCAN(maxlen_sentence, maxlen_word, max_features, embedding_dims)\n",
        "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[early_stopping],\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "print('Test...')\n",
        "result = model.predict(x_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Epoch 1/5\n",
            "520/520 [==============================] - 50s 63ms/step - loss: 0.1298 - accuracy: 0.9447 - val_loss: 0.0664 - val_accuracy: 0.9762\n",
            "Epoch 2/5\n",
            "520/520 [==============================] - 29s 57ms/step - loss: 0.0257 - accuracy: 0.9921 - val_loss: 0.0513 - val_accuracy: 0.9784\n",
            "Epoch 3/5\n",
            "520/520 [==============================] - 29s 57ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.1245 - val_accuracy: 0.9659\n",
            "Epoch 4/5\n",
            "520/520 [==============================] - 29s 57ms/step - loss: 0.0105 - accuracy: 0.9959 - val_loss: 0.0571 - val_accuracy: 0.9815\n",
            "Epoch 5/5\n",
            "520/520 [==============================] - 30s 58ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0625 - val_accuracy: 0.9825\n",
            "Test...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE8iLpZjxlxK"
      },
      "source": [
        "result = (result.reshape(1,-1)[0]>0.5).astype(int)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvGfhGdniXhA",
        "outputId": "e610be7b-e5d9-4b05-910b-30c00742d3b7"
      },
      "source": [
        "print(\"ACC\", metrics.accuracy_score(result, y_test))\n",
        "print(\"Prec\", metrics.precision_score(result, y_test))\n",
        "print(\"REC\", metrics.recall_score(result, y_test))\n",
        "print(\"F1\", metrics.f1_score(result, y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC 0.9824519230769231\n",
            "Prec 0.9863272041489863\n",
            "REC 0.9794007490636704\n",
            "F1 0.982851773549448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pocyY-6ZRmZx"
      },
      "source": [
        "**Saving Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQEe-cLDQdc7"
      },
      "source": [
        "model.save_weights(\"_hcan_\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHOqhaoORp0_"
      },
      "source": [
        "**Load Saved Model weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNpko48jRLbL"
      },
      "source": [
        "model2 = HCAN(maxlen_sentence, maxlen_word, max_features, embedding_dims)\n",
        "model2.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYhCd_rVQzzk",
        "outputId": "6616c06d-8c84-44f2-e723-7ce3ddfe9aed"
      },
      "source": [
        "model2.load_weights(\"_hcan_\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbebf93de50>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tgMG1JpRRaY"
      },
      "source": [
        "result = model2.predict(x_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rBbAY6yRUkR",
        "outputId": "0d7ceaa8-c07a-4b10-d0de-d99eef5beec4"
      },
      "source": [
        "result = (result.reshape(1,-1)[0]>0.5).astype(int)\n",
        "print(\"ACC\", metrics.accuracy_score(result, y_test))\n",
        "print(\"Prec\", metrics.precision_score(result, y_test))\n",
        "print(\"REC\", metrics.recall_score(result, y_test))\n",
        "print(\"F1\", metrics.f1_score(result, y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC 0.9824519230769231\n",
            "Prec 0.9863272041489863\n",
            "REC 0.9794007490636704\n",
            "F1 0.982851773549448\n"
          ]
        }
      ]
    }
  ]
}